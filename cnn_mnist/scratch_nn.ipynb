{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (28, 28)\n",
    "EPOCH = 5\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv(r\"../digit-recognizer/train.csv\")\n",
    "data_test = pd.read_csv(r\"../digit-recognizer/test.csv\")    \n",
    "\n",
    "# Get labels and image array from data\n",
    "labels : np.ndarray = data.values[:, 0]\n",
    "images : np.ndarray = data.values[:, 1:].astype('uint8')\n",
    "\n",
    "images_test : np.ndarray = data_test.values.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input, hidden, output, lr):\n",
    "        # Initialize weight and bias\n",
    "        self.w_input = np.random.normal(size=((input, output)))\n",
    "        # self.b_input = np.zeros(output)\n",
    "        # self.w_hidden = np.random.rand(hidden, output)\n",
    "        # self.b_hidden = np.random.rand(output)\n",
    "        \n",
    "        self.lr = lr\n",
    "\n",
    "    def sigmoid(self, x, derive=False):\n",
    "        if derive:\n",
    "            return self.sigmoid(x) * (1-self.sigmoid(x))\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x : np.ndarray):\n",
    "        new_x : np.ndarray = np.zeros(10)\n",
    "        x = np.subtract(x, np.max(x))\n",
    "        for i in range(10):\n",
    "            new_x[i] = np.exp(x[i]) / np.sum(np.exp(x), axis=0)\n",
    "            # print(np.exp(x[i]))\n",
    "        # print(new_x)\n",
    "        return new_x \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.dot(x, self.w_input)\n",
    "        # x = self.sigmoid(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def train(self, input: np.ndarray, pred: np.ndarray, label: np.ndarray, yes=False):\n",
    "        # ============================================\n",
    "        # # sigmoid with MSELoss (Mean Squared Error)\n",
    "        # z = weight * input\n",
    "        # pred = sigmoid(z) {a(L)}\n",
    "        # cost = (pred-label)**2\n",
    "        # z = np.dot(input, self.w_input)\n",
    "        # d_pred = self.sigmoid(z, derive=True)     \n",
    "        # d_cost = -2*(pred-label)\n",
    "        \n",
    "        # # Weight input gradient descent\n",
    "        # # y1 = w_input * input = z\n",
    "        # step = z * d_pred  * d_cost * self.lr\n",
    "        # np.subtract(self.w_input, step, out=self.w_input)\n",
    "        # ============================================\n",
    "        \n",
    "        # ============================================\n",
    "        # softmax with Cross Entropy Loss\n",
    "        \n",
    "        # label only have one 1 value (the correct one), so technically, -ln(pred) is also correct\n",
    "        # cost = -sum(label * ln(pred)) = -ln(pred)\n",
    "        # pred = softmax(out)\n",
    "        # out = w * input\n",
    "        # d_cost = -1 / (pred)\n",
    "        # d_pred = pred * (1 - pred) # From StatQuest\n",
    "        # d_cost * d_pred = pred[i] - 1\n",
    "        # self.w_input[j, i] -= step\n",
    "\n",
    "        d_cost = -1 / pred[np.argmax(label)] # d_cost\n",
    "        for i, p in enumerate(pred): # d_pred (derivative of softmax)\n",
    "            # d_softmax/d_y1 = (e**y1 * (e**y2 + e**y3 + ...)) / (e**y1 + e**y2 + e**y3 + ...)**2\n",
    "            d_pred = np.exp(p) * np.sum(np.exp(pred[pred != p])) / np.sum(np.exp(pred))**2\n",
    "            for j in range(len(input)):\n",
    "                d_x = input[j]\n",
    "                self.w_input[j, i] += d_cost * d_pred * d_x * self.lr\n",
    "\n",
    "        if (yes):\n",
    "            print(\"Cost: \", np.log(pred[np.argmax(label)]))\n",
    "            # print(\"Step: \", step)\n",
    "            # print(\"d_pred: \", d_pred)\n",
    "            # print(\"d_cost: \", d_cost)\n",
    "            \n",
    "        # ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data 2000: Wrong = 1520, Accuracy: 23.96198099049525%\n",
      "Data 4000: Wrong = 2929, Accuracy: 26.756689172293065%\n",
      "Data 6000: Wrong = 4348, Accuracy: 27.521253542257043%\n",
      "Data 8000: Wrong = 5749, Accuracy: 28.128516064508062%\n",
      "Data 10000: Wrong = 7082, Accuracy: 29.172917291729178%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18624\\2598594719.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# 0 will also break log e, because e sucks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18624\\979985042.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;31m# x = self.sigmoid(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(IMAGE_SIZE[0]*IMAGE_SIZE[1], 16, 10, LEARNING_RATE)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    wrong = 0\n",
    "    for j, d in enumerate(images):\n",
    "        # High input breaks the neural network\n",
    "        # Problem such as NaN, inf, etc, because e sucks\n",
    "        d = d / 255\n",
    "        \n",
    "        pred = model.forward(d)\n",
    "        \n",
    "        # 0 will also break log e, because e sucks\n",
    "        # Clip pred so the value only ranges around 1e-7 and 1\n",
    "        pred = np.clip(pred, 1e-7, 1)\n",
    "\n",
    "        if (np.argmax(pred) != labels[j]):\n",
    "            wrong += 1\n",
    "            \n",
    "        label = np.zeros(10)\n",
    "        label[labels[j]] = 1\n",
    "        model.train(d, pred, label)\n",
    "\n",
    "        if ((j+1) % 2000 == 0):\n",
    "            # print(model.w_input)\n",
    "            # model.train(d, pred, label, True)\n",
    "            print(f\"Data {j+1}: Wrong = {wrong}, Accuracy: {100-wrong/j*100}%\")\n",
    "\n",
    "    print(f\"Epoch: {i+1} --> Wrong: {wrong}, Accuracy: {100-wrong / labels.size * 100}%\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing = np.array([\n",
    "#     [1, 1, 1],\n",
    "#     [1, 0, 1],\n",
    "#     [0, 1, 1],\n",
    "#     [0, 0, 1],\n",
    "#     [0, 0, 0],\n",
    "#     [1, 0, 0],\n",
    "#     [1, 1, 0]\n",
    "# ])\n",
    "\n",
    "# label = np.array(\n",
    "#     [1, 1, 1, 0, 0, 0, 1]\n",
    "# )\n",
    "\n",
    "# model = Model(3, 0, 1, LEARNING_RATE)\n",
    "# for i, d in enumerate(testing):\n",
    "#     res = float(model.forward(d))\n",
    "    \n",
    "#     print(res)\n",
    "#     model.train(d, res, label[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad0a7ea84f13c5490ff16707edd4f345ddc4e5570f1b3225d67fb71a4ba2808c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
