{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (28, 28)\n",
    "EPOCH = 1000\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "# Import data\n",
    "data = pd.read_csv(r\"../digit-recognizer/train.csv\")\n",
    "data_test = pd.read_csv(r\"../digit-recognizer/test.csv\")    \n",
    "\n",
    "# Get labels and image array from data\n",
    "labels : np.ndarray = data.values[:, 0]\n",
    "images : np.ndarray = data.values[:, 1:].astype('uint8')\n",
    "\n",
    "images_test : np.ndarray = data_test.values.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference for backpropagation (from 3b1b neural network video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> SOFTMAX MODEL </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSoftmax:\n",
    "    def __init__(self, input, hidden, output, lr):\n",
    "        # Initialize weight and bias\n",
    "        self.w_input = np.random.normal(size=((input, output)))\n",
    "        # self.b_input = np.zeros(output)\n",
    "        # self.w_hidden = np.random.rand(hidden, output)\n",
    "        # self.b_hidden = np.random.rand(output)\n",
    "        \n",
    "        self.lr = lr\n",
    "\n",
    "    def softmax(self, x : np.ndarray):\n",
    "        new_x : np.ndarray = np.zeros(10)\n",
    "        x = np.subtract(x, np.max(x))\n",
    "        for i in range(10):\n",
    "            new_x[i] = np.exp(x[i]) / np.sum(np.exp(x), axis=0)\n",
    "            # print(np.exp(x[i]))\n",
    "        # print(new_x)\n",
    "        return new_x \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.dot(x, self.w_input)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def train(self, input: np.ndarray, pred: np.ndarray, label: np.ndarray, debug=False):\n",
    "        # ============================================\n",
    "        # Softmax with Cross Entropy Loss\n",
    "        \n",
    "        # cost = -sum(label * ln(pred)) = -ln(pred)\n",
    "        # label array only has one 1 value (the correct label), so technically, -ln(pred) is also correct\n",
    "        # pred = softmax(out)\n",
    "        # out = w * input\n",
    "        # d_cost = -1 / (pred)\n",
    "        # d_pred = pred * (1 - pred) # From StatQuest\n",
    "        # d_cost * d_pred = pred[i] - 1\n",
    "        # self.w_input[j, i] -= step\n",
    "\n",
    "        d_cost = -1 / pred[np.argmax(label)] # d_cost\n",
    "        d_pred = np.zeros(len(pred))\n",
    "        for i, p in enumerate(pred): # d_pred (derivative of softmax)\n",
    "            # d_softmax/d_y1 = (e**y1 * (e**y2 + e**y3 + ...)) / (e**y1 + e**y2 + e**y3 + ...)**2\n",
    "            d_pred[i] = np.exp(p) * np.sum(np.exp(pred[pred != p])) / np.sum(np.exp(pred))**2\n",
    "            \n",
    "        step = np.outer(input, d_pred)  * d_cost * self.lr\n",
    "        self.w_input += step\n",
    "\n",
    "        # Debug\n",
    "        if (debug):\n",
    "            print(\"Cost: \", np.log(pred[np.argmax(label)]))\n",
    "            # print(\"Step: \", step)\n",
    "            # print(\"d_pred: \", d_pred)\n",
    "            # print(\"d_cost: \", d_cost)\n",
    "            \n",
    "        # ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> SIGMOID MODEL </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSigmoid:\n",
    "    def __init__(self, input, hidden, output, lr):\n",
    "        # Initialize weight and bias\n",
    "        self.w_input = np.random.normal(size=((input, output)))\n",
    "        # self.b_input = np.zeros(output)\n",
    "        # self.w_hidden = np.random.rand(hidden, output)\n",
    "        # self.b_hidden = np.random.rand(output)\n",
    "        \n",
    "        self.lr = lr\n",
    "\n",
    "    def sigmoid(self, x, derive=False):\n",
    "        if derive:\n",
    "            return self.sigmoid(x) * (1-self.sigmoid(x))\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.dot(x, self.w_input)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def train(self, input: np.ndarray, pred: np.ndarray, label: np.ndarray, debug=False):\n",
    "        # ============================================\n",
    "        # # sigmoid with MSELoss (Mean Squared Error)\n",
    "        # cost = 1/len(pred) * np.sum(pred-label)**2\n",
    "        # d_cost = 2/len(pred) * (pred-label)\n",
    "        \n",
    "        # pred = sigmoid(z) {a(L)}\n",
    "        # d_pred = self.sigmoid(z, derive=True)     \n",
    "        \n",
    "        # z = weight * input\n",
    "        # z = np.dot(input, self.w_input)\n",
    "        \n",
    "        # Weight input gradient descent\n",
    "        d_cost = 2/len(pred) * (pred-label)\n",
    "        d_pred = self.sigmoid(np.dot(input, self.w_input), derive=True)\n",
    "        step = np.outer(input, d_pred) * d_cost * self.lr\n",
    "        self.w_input -= step\n",
    "\n",
    "        # Debug\n",
    "        if (debug):\n",
    "            print(\"Cost: \", np.sum(pred-label)**2/len(pred))\n",
    "            # print(\"Step: \", step)\n",
    "            # print(\"d_pred: \", d_pred)\n",
    "            # print(\"d_cost: \", d_cost)\n",
    "            \n",
    "        # ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  3.2860245986052226\n",
      "Data 2000: Wrong = 1762, Accuracy: 11.855927963981998%\n",
      "Cost:  2.518142630297416\n",
      "Data 4000: Wrong = 3533, Accuracy: 11.65291322830707%\n",
      "Cost:  1.7205573037879134\n",
      "Data 6000: Wrong = 5319, Accuracy: 11.335222537089521%\n",
      "Cost:  1.490027120166312\n",
      "Data 8000: Wrong = 7091, Accuracy: 11.351418927365913%\n",
      "Cost:  2.6703658406692634\n",
      "Data 10000: Wrong = 8860, Accuracy: 11.391139113911379%\n",
      "Cost:  1.6418139012858632\n",
      "Data 12000: Wrong = 10640, Accuracy: 11.325943828652385%\n",
      "Cost:  0.9643193167785201\n",
      "Data 14000: Wrong = 12435, Accuracy: 11.172226587613395%\n",
      "Cost:  2.4605124839888584\n",
      "Data 16000: Wrong = 14236, Accuracy: 11.019438714919687%\n",
      "Cost:  2.0225155988529453\n",
      "Data 18000: Wrong = 15999, Accuracy: 11.111728429357186%\n",
      "Cost:  2.3450114262919604\n",
      "Data 20000: Wrong = 17766, Accuracy: 11.165558277913902%\n",
      "Cost:  3.6784308265262853\n",
      "Data 22000: Wrong = 19540, Accuracy: 11.177780808218557%\n",
      "Cost:  1.050102428605122\n",
      "Data 24000: Wrong = 21314, Accuracy: 11.187966165256896%\n",
      "Cost:  2.7536885037106806\n",
      "Data 26000: Wrong = 23071, Accuracy: 11.261971614292861%\n",
      "Cost:  3.1493038720551376\n",
      "Data 28000: Wrong = 24831, Accuracy: 11.314689810350359%\n",
      "Cost:  2.122796025197562\n",
      "Data 30000: Wrong = 26610, Accuracy: 11.297043234774492%\n",
      "Cost:  2.4343028674010663\n",
      "Data 32000: Wrong = 28376, Accuracy: 11.322228819650618%\n",
      "Cost:  0.8240862881246758\n",
      "Data 34000: Wrong = 30180, Accuracy: 11.232683314215123%\n",
      "Cost:  4.734486582007229\n",
      "Data 36000: Wrong = 31981, Accuracy: 11.161421150587515%\n",
      "Cost:  1.0623507922313264\n",
      "Data 38000: Wrong = 33729, Accuracy: 11.237137819416304%\n",
      "Cost:  3.0457067201159975\n",
      "Data 40000: Wrong = 35500, Accuracy: 11.247781194529864%\n",
      "Cost:  3.054283338578922\n",
      "Data 42000: Wrong = 37267, Accuracy: 11.266934927022078%\n",
      "Epoch: 1 --> Wrong: 37267, Accuracy: 11.269047619047626%\n",
      "\n",
      "Cost:  3.2611402937759926\n",
      "Data 2000: Wrong = 1762, Accuracy: 11.855927963981998%\n",
      "Cost:  2.472642610567446\n",
      "Data 4000: Wrong = 3532, Accuracy: 11.67791947986997%\n",
      "Cost:  1.7124690097660067\n",
      "Data 6000: Wrong = 5318, Accuracy: 11.351891981997%\n",
      "Cost:  1.4723644252877457\n",
      "Data 8000: Wrong = 7091, Accuracy: 11.351418927365913%\n",
      "Cost:  2.650850480439087\n",
      "Data 10000: Wrong = 8860, Accuracy: 11.391139113911379%\n",
      "Cost:  1.630892377243806\n",
      "Data 12000: Wrong = 10640, Accuracy: 11.325943828652385%\n",
      "Cost:  0.9602215466503659\n",
      "Data 14000: Wrong = 12435, Accuracy: 11.172226587613395%\n",
      "Cost:  2.4593006363982743\n",
      "Data 16000: Wrong = 14235, Accuracy: 11.0256891055691%\n",
      "Cost:  2.002121231313657\n",
      "Data 18000: Wrong = 15998, Accuracy: 11.117284293571856%\n",
      "Cost:  2.3354205313091927\n",
      "Data 20000: Wrong = 17765, Accuracy: 11.170558527926403%\n",
      "Cost:  3.640836595332653\n",
      "Data 22000: Wrong = 19539, Accuracy: 11.182326469384975%\n",
      "Cost:  1.0420161791572138\n",
      "Data 24000: Wrong = 21312, Accuracy: 11.1962998458269%\n",
      "Cost:  2.732736477525941\n",
      "Data 26000: Wrong = 23068, Accuracy: 11.273510519635366%\n",
      "Cost:  3.1314340647411183\n",
      "Data 28000: Wrong = 24827, Accuracy: 11.328976034858385%\n",
      "Cost:  2.09592475390119\n",
      "Data 30000: Wrong = 26606, Accuracy: 11.310377012567088%\n",
      "Cost:  2.424031160498959\n",
      "Data 32000: Wrong = 28372, Accuracy: 11.334729210287819%\n",
      "Cost:  0.8152094859260725\n",
      "Data 34000: Wrong = 30175, Accuracy: 11.247389629106735%\n",
      "Cost:  4.702922830772121\n",
      "Data 36000: Wrong = 31977, Accuracy: 11.172532570349176%\n",
      "Cost:  1.0539762628735994\n",
      "Data 38000: Wrong = 33726, Accuracy: 11.245032764020095%\n",
      "Cost:  3.0243612005280136\n",
      "Data 40000: Wrong = 35498, Accuracy: 11.252781319532986%\n",
      "Cost:  3.0380106643696996\n",
      "Data 42000: Wrong = 37265, Accuracy: 11.27169694516536%\n",
      "Epoch: 2 --> Wrong: 37265, Accuracy: 11.273809523809518%\n",
      "\n",
      "Cost:  3.2356961970776936\n",
      "Data 2000: Wrong = 1762, Accuracy: 11.855927963981998%\n",
      "Cost:  2.4266395239887326\n",
      "Data 4000: Wrong = 3531, Accuracy: 11.702925731432856%\n",
      "Cost:  1.704860321971062\n",
      "Data 6000: Wrong = 5318, Accuracy: 11.351891981997%\n",
      "Cost:  1.4547909419008174\n",
      "Data 8000: Wrong = 7091, Accuracy: 11.351418927365913%\n",
      "Cost:  2.631173800269676\n",
      "Data 10000: Wrong = 8860, Accuracy: 11.391139113911379%\n",
      "Cost:  1.6202797768016992\n",
      "Data 12000: Wrong = 10639, Accuracy: 11.334277856488043%\n",
      "Cost:  0.9563869856093726\n",
      "Data 14000: Wrong = 12434, Accuracy: 11.179369954996787%\n",
      "Cost:  2.458088284562137\n",
      "Data 16000: Wrong = 14234, Accuracy: 11.031939496218513%\n",
      "Cost:  1.9823814887637723\n",
      "Data 18000: Wrong = 15998, Accuracy: 11.117284293571856%\n",
      "Cost:  2.3256039846891357\n",
      "Data 20000: Wrong = 17765, Accuracy: 11.170558527926403%\n",
      "Cost:  3.603189509572922\n",
      "Data 22000: Wrong = 19538, Accuracy: 11.186872130551379%\n",
      "Cost:  1.03406028020745\n",
      "Data 24000: Wrong = 21311, Accuracy: 11.200466686111923%\n",
      "Cost:  2.7115643570029184\n",
      "Data 26000: Wrong = 23067, Accuracy: 11.277356821416205%\n",
      "Cost:  3.112950039498\n",
      "Data 28000: Wrong = 24826, Accuracy: 11.3325475909854%\n",
      "Cost:  2.0695305724470785\n",
      "Data 30000: Wrong = 26605, Accuracy: 11.313710457015233%\n",
      "Cost:  2.4129019043821285\n",
      "Data 32000: Wrong = 28371, Accuracy: 11.337854307947126%\n",
      "Cost:  0.8064952399829677\n",
      "Data 34000: Wrong = 30174, Accuracy: 11.25033089208506%\n",
      "Cost:  4.671122725304295\n",
      "Data 36000: Wrong = 31977, Accuracy: 11.172532570349176%\n",
      "Cost:  1.046005811484057\n",
      "Data 38000: Wrong = 33726, Accuracy: 11.245032764020095%\n",
      "Cost:  3.002628716153249\n",
      "Data 40000: Wrong = 35498, Accuracy: 11.252781319532986%\n",
      "Cost:  3.021396621991405\n",
      "Data 42000: Wrong = 37264, Accuracy: 11.274077954237%\n",
      "Epoch: 3 --> Wrong: 37264, Accuracy: 11.276190476190479%\n",
      "\n",
      "Cost:  3.2097060832633204\n",
      "Data 2000: Wrong = 1762, Accuracy: 11.855927963981998%\n",
      "Cost:  2.3802210483884023\n",
      "Data 4000: Wrong = 3531, Accuracy: 11.702925731432856%\n",
      "Cost:  1.6977053470955294\n",
      "Data 6000: Wrong = 5317, Accuracy: 11.368561426904492%\n",
      "Cost:  1.4372961748428366\n",
      "Data 8000: Wrong = 7090, Accuracy: 11.363920490061247%\n",
      "Cost:  2.6113404313716173\n",
      "Data 10000: Wrong = 8861, Accuracy: 11.381138113811389%\n",
      "Cost:  1.609915583291409\n",
      "Data 12000: Wrong = 10642, Accuracy: 11.309275772981081%\n",
      "Cost:  0.9527994678794125\n",
      "Data 14000: Wrong = 12438, Accuracy: 11.150796485463246%\n",
      "Cost:  2.456873339744886\n",
      "Data 16000: Wrong = 14237, Accuracy: 11.01318832427026%\n",
      "Cost:  1.9632552273921466\n",
      "Data 18000: Wrong = 16001, Accuracy: 11.10061670092783%\n",
      "Cost:  2.3155627841319553\n",
      "Data 20000: Wrong = 17770, Accuracy: 11.145557277863887%\n",
      "Cost:  3.5654954416928595\n",
      "Data 22000: Wrong = 19543, Accuracy: 11.164143824719304%\n",
      "Cost:  1.0262337085091162\n",
      "Data 24000: Wrong = 21316, Accuracy: 11.179632484686863%\n",
      "Cost:  2.6901792192969327\n",
      "Data 26000: Wrong = 23072, Accuracy: 11.258125312512021%\n",
      "Cost:  3.0938408484223543\n",
      "Data 28000: Wrong = 24830, Accuracy: 11.318261366477373%\n",
      "Cost:  2.043614716024638\n",
      "Data 30000: Wrong = 26609, Accuracy: 11.300376679222651%\n",
      "Cost:  2.4008340048642838\n",
      "Data 32000: Wrong = 28375, Accuracy: 11.325353917309911%\n",
      "Cost:  0.7979304976547693\n",
      "Data 34000: Wrong = 30178, Accuracy: 11.23856584017176%\n",
      "Cost:  4.639089121409561\n",
      "Data 36000: Wrong = 31982, Accuracy: 11.158643295647096%\n",
      "Cost:  1.0384234811910462\n",
      "Data 38000: Wrong = 33732, Accuracy: 11.229242874812499%\n",
      "Cost:  2.980519709709658\n",
      "Data 40000: Wrong = 35505, Accuracy: 11.235280882022053%\n",
      "Cost:  3.004436738842628\n",
      "Data 42000: Wrong = 37272, Accuracy: 11.255029881663845%\n",
      "Epoch: 4 --> Wrong: 37272, Accuracy: 11.257142857142853%\n",
      "\n",
      "Cost:  3.183189327663495\n",
      "Data 2000: Wrong = 1763, Accuracy: 11.80590295147573%\n",
      "Cost:  2.333477423775696\n",
      "Data 4000: Wrong = 3533, Accuracy: 11.65291322830707%\n",
      "Cost:  1.690978487712491\n",
      "Data 6000: Wrong = 5318, Accuracy: 11.351891981997%\n",
      "Cost:  1.419873210001132\n",
      "Data 8000: Wrong = 7092, Accuracy: 11.338917364670593%\n",
      "Cost:  2.591355024681728\n",
      "Data 10000: Wrong = 8862, Accuracy: 11.37113711371137%\n",
      "Cost:  1.599741785675945\n",
      "Data 12000: Wrong = 10644, Accuracy: 11.292607717309778%\n",
      "Cost:  0.9494435399980319\n",
      "Data 14000: Wrong = 12440, Accuracy: 11.136509750696476%\n",
      "Cost:  2.4556538015107394\n",
      "Data 16000: Wrong = 14239, Accuracy: 11.000687542971434%\n",
      "Cost:  1.944702133092673\n",
      "Data 18000: Wrong = 16002, Accuracy: 11.09506083671316%\n",
      "Cost:  2.3052985715759227\n",
      "Data 20000: Wrong = 17770, Accuracy: 11.145557277863887%\n",
      "Cost:  3.527760637828309\n",
      "Data 22000: Wrong = 19543, Accuracy: 11.164143824719304%\n",
      "Cost:  1.0185352943734265\n",
      "Data 24000: Wrong = 21317, Accuracy: 11.175465644401854%\n",
      "Cost:  2.6685889923914266\n",
      "Data 26000: Wrong = 23074, Accuracy: 11.250432708950342%\n",
      "Cost:  3.0740964233474166\n",
      "Data 28000: Wrong = 24831, Accuracy: 11.314689810350359%\n",
      "Cost:  2.018176511017692\n",
      "Data 30000: Wrong = 26611, Accuracy: 11.293709790326346%\n",
      "Cost:  2.3877491127495434\n",
      "Data 32000: Wrong = 28379, Accuracy: 11.312853526672711%\n",
      "Cost:  0.7895030603038087\n",
      "Data 34000: Wrong = 30182, Accuracy: 11.226800788258473%\n",
      "Cost:  4.606826045501995\n",
      "Data 36000: Wrong = 31986, Accuracy: 11.147531875885448%\n",
      "Cost:  1.0312131406668334\n",
      "Data 38000: Wrong = 33736, Accuracy: 11.21871628200742%\n",
      "Cost:  2.958045890304717\n",
      "Data 40000: Wrong = 35509, Accuracy: 11.225280632015796%\n",
      "Cost:  2.987126853888977\n",
      "Data 42000: Wrong = 37276, Accuracy: 11.245505845377266%\n",
      "Epoch: 5 --> Wrong: 37276, Accuracy: 11.247619047619054%\n",
      "\n",
      "Cost:  3.1561706208020492\n",
      "Data 2000: Wrong = 1762, Accuracy: 11.855927963981998%\n",
      "Cost:  2.286500625314287\n",
      "Data 4000: Wrong = 3532, Accuracy: 11.67791947986997%\n",
      "Cost:  1.684654624968564\n",
      "Data 6000: Wrong = 5317, Accuracy: 11.368561426904492%\n",
      "Cost:  1.4025188895677687\n",
      "Data 8000: Wrong = 7090, Accuracy: 11.363920490061247%\n",
      "Cost:  2.5712222425459808\n",
      "Data 10000: Wrong = 8861, Accuracy: 11.381138113811389%\n",
      "Cost:  1.5897027877302592\n",
      "Data 12000: Wrong = 10644, Accuracy: 11.292607717309778%\n",
      "Cost:  0.9463044811040838\n",
      "Data 14000: Wrong = 12441, Accuracy: 11.129366383313084%\n",
      "Cost:  2.454427749276519\n",
      "Data 16000: Wrong = 14239, Accuracy: 11.000687542971434%\n",
      "Cost:  1.9266826994567487\n",
      "Data 18000: Wrong = 16003, Accuracy: 11.089504972498474%\n",
      "Cost:  2.2948136395167302\n",
      "Data 20000: Wrong = 17770, Accuracy: 11.145557277863887%\n",
      "Cost:  3.489991580727483\n",
      "Data 22000: Wrong = 19544, Accuracy: 11.159598163552886%\n",
      "Cost:  1.0109637285768378\n",
      "Data 24000: Wrong = 21318, Accuracy: 11.17129880411683%\n",
      "Cost:  2.6468023883644722\n",
      "Data 26000: Wrong = 23075, Accuracy: 11.246586407169502%\n",
      "Cost:  3.0537076697717915\n",
      "Data 28000: Wrong = 24834, Accuracy: 11.30397514196936%\n",
      "Cost:  1.993213336179333\n",
      "Data 30000: Wrong = 26614, Accuracy: 11.283709456981896%\n",
      "Cost:  2.3735728272098653\n",
      "Data 32000: Wrong = 28382, Accuracy: 11.303478233694804%\n",
      "Cost:  0.7812016137072951\n",
      "Data 34000: Wrong = 30185, Accuracy: 11.217976999323511%\n",
      "Cost:  4.574338750201571\n",
      "Data 36000: Wrong = 31991, Accuracy: 11.133642601183368%\n",
      "Cost:  1.024358585867888\n",
      "Data 38000: Wrong = 33739, Accuracy: 11.210821337403615%\n",
      "Cost:  2.935220097748098\n",
      "Data 40000: Wrong = 35512, Accuracy: 11.21778044451112%\n",
      "Cost:  2.9694631059959113\n",
      "Data 42000: Wrong = 37278, Accuracy: 11.240743827233985%\n",
      "Epoch: 6 --> Wrong: 37278, Accuracy: 11.242857142857147%\n",
      "\n",
      "Cost:  3.128679601853136\n",
      "Data 2000: Wrong = 1760, Accuracy: 11.95597798899449%\n",
      "Cost:  2.239383516392769\n",
      "Data 4000: Wrong = 3530, Accuracy: 11.727931982995756%\n",
      "Cost:  1.6787092649925537\n",
      "Data 6000: Wrong = 5315, Accuracy: 11.401900316719448%\n",
      "Cost:  1.385233910603756\n",
      "Data 8000: Wrong = 7087, Accuracy: 11.401425178147278%\n",
      "Cost:  2.550946750150996\n",
      "Data 10000: Wrong = 8856, Accuracy: 11.431143114311425%\n",
      "Cost:  1.5797453278153324\n",
      "Data 12000: Wrong = 10638, Accuracy: 11.342611884323688%\n",
      "Cost:  0.943368311018735\n",
      "Data 14000: Wrong = 12437, Accuracy: 11.157939852846638%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8688\\3171533996.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8688\\4126272620.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input, pred, label, debug)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0md_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_cost\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_input\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Debug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ModelSoftmax(IMAGE_SIZE[0]*IMAGE_SIZE[1], 16, 10, LEARNING_RATE)\n",
    "# model = ModelSigmoid(IMAGE_SIZE[0]*IMAGE_SIZE[1], 16, 10, LEARNING_RATE)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    wrong = 0\n",
    "    for j, d in enumerate(images):\n",
    "        # High input breaks the neural network\n",
    "        # Problem such as NaN, inf, etc, because e sucks\n",
    "        d = d / 255\n",
    "        \n",
    "        pred = model.forward(d)\n",
    "        \n",
    "        # 0 will also break log e, because e sucks\n",
    "        # Clip pred so the value only ranges around 1e-7 and 1\n",
    "        pred = np.clip(pred, 1e-7, 1e7)\n",
    "\n",
    "        if (np.argmax(pred) != labels[j]):\n",
    "            wrong += 1\n",
    "            \n",
    "        label = np.zeros(10)\n",
    "        label[labels[j]] = 1\n",
    "        model.train(d, pred, label)\n",
    "\n",
    "        if ((j+1) % 2000 == 0):\n",
    "            # print(model.w_input)\n",
    "            model.train(d, pred, label, True)\n",
    "            print(f\"Data {j+1}: Wrong = {wrong}, Accuracy: {100-wrong/j*100}%\")\n",
    "\n",
    "    print(f\"Epoch: {i+1} --> Wrong: {wrong}, Accuracy: {100-wrong / labels.size * 100}%\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing = np.array([\n",
    "#     [1, 1, 1],\n",
    "#     [1, 0, 1],\n",
    "#     [0, 1, 1],\n",
    "#     [0, 0, 1],\n",
    "#     [0, 0, 0],\n",
    "#     [1, 0, 0],\n",
    "#     [1, 1, 0]\n",
    "# ])\n",
    "\n",
    "# label = np.array(\n",
    "#     [1, 1, 1, 0, 0, 0, 1]\n",
    "# )\n",
    "\n",
    "# model = Model(3, 0, 1, LEARNING_RATE)\n",
    "# for i, d in enumerate(testing):\n",
    "#     res = float(model.forward(d))\n",
    "    \n",
    "#     print(res)\n",
    "#     model.train(d, res, label[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad0a7ea84f13c5490ff16707edd4f345ddc4e5570f1b3225d67fb71a4ba2808c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
